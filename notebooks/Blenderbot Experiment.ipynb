{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f583e85d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mj/8vz4yhqj5w76hnkw54qktw900000gn/T/ipykernel_208/3664963009.py:4: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML, Markdown\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#notebook { padding-top:10px !important; } .container { width:95% !important; } .end_space { min-height:5px !important; } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# This gets rid of Jupyter's screen-realestate-wating-margin-of-gray.\n",
    "#\n",
    "\n",
    "from IPython.core.display import display, HTML, Markdown\n",
    "display(HTML(\n",
    "    '<style>'\n",
    "        '#notebook { padding-top:10px !important; } ' \n",
    "        '.container { width:95% !important; } '\n",
    "        '.end_space { min-height:5px !important; } '\n",
    "    '</style>'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79e04c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.10.7'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from platform import python_version \n",
    "python_version()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d39da7b",
   "metadata": {},
   "source": [
    "# Customize Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cf04d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust these directories to where you put your stuff\n",
    "import os\n",
    "\n",
    "commonsRoot = os.path.expanduser(\"~/IdeaProjects/commons\")                           \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd58230",
   "metadata": {},
   "source": [
    "# Set up Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71450fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Include the Python codes in the recommendations project into Jupyter's libraries\n",
    "#\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "for d in [commonsRoot, ]:\n",
    "    sourceDir = d + '/src/main'\n",
    "    if sourceDir not in sys.path:\n",
    "        sys.path.append(sourceDir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7102baa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b952848",
   "metadata": {},
   "source": [
    "# Experiment with Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7a90b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlenderbotTokenizer, BlenderbotForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40fc785c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b180377de3334af39eb7475821df8d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e84e5762ae084a27af397347e56d23a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c15c07480c4cc9b8487e6f79695dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/124k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c450815bf1b14998bb90110de1204831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/61.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36047fd69eb24db1af7ca03daa769d9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dce284346a114f7198255cd5bd4fe861",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/16.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dd8dc3277884ab0ac6d80513e078661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/772 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<s> That's unfortunate. Are they trying to lose weight or are they just trying to be healthier?</s>\"]\n",
      "7.457543134689331\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# mname = \"facebook/blenderbot-400M-distill\"\n",
    "mname = \"facebook/blenderbot-1B-distill\"\n",
    "model = BlenderbotForConditionalGeneration.from_pretrained(mname)\n",
    "tokenizer = BlenderbotTokenizer.from_pretrained(mname)\n",
    "UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n",
    "\n",
    "statTime = time.time()\n",
    "inputs = tokenizer([UTTERANCE], return_tensors=\"pt\")\n",
    "reply_ids = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(reply_ids))\n",
    "print(time.time() - statTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b68c2e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<s> That's what I was thinking, but I don't know if I want to risk it.</s>\"]\n",
      "7.560004711151123\n"
     ]
    }
   ],
   "source": [
    "statTime = time.time()\n",
    "UTTERANCE = \"no, they just like sweets\"\n",
    "inputs = tokenizer([UTTERANCE], return_tensors=\"pt\")\n",
    "reply_ids = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(reply_ids))\n",
    "print(time.time() - statTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6ea0989e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"<s> I love it! It's one of the most popular foods in the world. What do you like to eat?</s>\"]\n",
      "7.227740049362183\n"
     ]
    }
   ],
   "source": [
    "statTime = time.time()\n",
    "# UTTERANCE = '''\n",
    "# your persona: I like cheese\n",
    "# your persona: I am from New York City\n",
    "# Hi, where are you from\n",
    "# i ' m from the city of new york . i love cheese . what do you like to eat ?\n",
    "# do you like cheese?\n",
    "# yes , i love it . it ' s one of my favorite foods . what ' s your favorite food ?\n",
    "# do you know what is piave?\n",
    "# '''\n",
    "UTTERANCE = '''\n",
    "you: I like cheese\n",
    "you: I am from New York City\n",
    "Hi, how are you?  Do you like Piave?\n",
    "'''\n",
    "\n",
    "inputs = tokenizer([UTTERANCE], return_tensors=\"pt\", padding=True)\n",
    "reply_ids = model.generate(**inputs)\n",
    "print(tokenizer.batch_decode(reply_ids))\n",
    "print(time.time() - statTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1766de9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 206,   96,  512,  800,   72,   33,  281,  398, 4686,  206,   96,  512,\n",
       "          800,   72,   33,  281,  632,  482, 2310, 6210, 5203,  206,  206,    2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93027e44",
   "metadata": {},
   "source": [
    "# Experiment with ParlAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8eee447",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "```\n",
    "pip install parlai\n",
    "pip install subword-nmt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ef5f51b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:08:48 | \u001b[33myour model is being loaded with opts that do not exist in the model you are initializing the weights with: evaltask: projects.bb3.tasks.module_level_tasks:AlwaysSearchTeacher,projects.bb3.tasks.module_level_tasks:MaybeSearchTeacher,projects.bb3.tasks.module_level_tasks:MemoryDecisionTeacher,projects.bb3.tasks.module_level_tasks:SearchQueryGenerationTeacher,projects.bb3.tasks.module_level_tasks:MemoryGenerationTeacher,projects.bb3.tasks.module_level_tasks:MemoryKnowledgeGenerationTeacher,projects.bb3.tasks.module_level_tasks:SearchKnowledgeGenerationTeacher,projects.bb3.tasks.module_level_tasks:EntityKnowledgeGenerationTeacher,projects.bb3.tasks.module_level_tasks:SearchDialogueGenerationTeacher,projects.bb3.tasks.module_level_tasks:EntityDialogueGenerationTeacher,projects.bb3.tasks.module_level_tasks:MemoryDialogueGenerationTeacher,projects.bb3.tasks.module_level_tasks:VanillaDialogueGenerationTeacher,final_extra_opt: ,eval_batchsize: None,eval_dynamic_batching: None,num_workers: 0,display_examples: False,num_epochs: -1,max_train_time: -1,max_train_steps: -1,log_every_n_steps: 50,validation_every_n_secs: -1,validation_every_n_steps: 1000,save_every_n_secs: -1,save_after_valid: True,validation_every_n_epochs: -1,validation_max_exs: 100000,short_final_eval: False,validation_patience: 10,validation_metric: ppl,validation_metric_mode: min,validation_cutoff: 1.0,validation_share_agent: False,metrics: default,aggregate_micro: False,world_logs: ,save_format: conversations,log_keep_fields: all,tensorboard_log: False,tensorboard_logdir: None,dict_maxexs: -1,dict_include_valid: False,dict_include_test: False,log_every_n_secs: -1,distributed_world_size: 64,ddp_backend: zero2,port: 36973,mutators: None,jsonfile_datapath: None,jsonfile_datatype_extension: False,label_turns: secondspeaker,filter_for_label: None,num_shards: 1,shard_id: 0,candidates: inline,eval_candidates: inline,interactive_candidates: fixed,repeat_blocking_heuristic: True,fixed_candidates_path: None,fixed_candidate_vecs: reuse,encode_candidate_vecs: True,encode_candidate_vecs_batchsize: 256,train_predict: False,cap_num_predictions: 100,ignore_bad_candidates: False,rank_top_k: -1,return_cand_scores: False,use_memories: False,wrap_memory_encoder: False,memory_attention: sqrt,normalize_sent_emb: False,share_encoders: True,learn_embeddings: True,data_parallel: False,reduction_type: mean,polyencoder_type: codes,poly_n_codes: 64,poly_attention_type: basic,poly_attention_num_heads: 4,codes_attention_type: basic,codes_attention_num_heads: 4,generation_model: bart,query_model: bert,rag_model_type: token,thorough: False,n_extra_positions: 0,gold_knowledge_passage_key: checked_sentence,gold_knowledge_title_key: title,rag_retriever_query: full_history,rag_retriever_type: observation_echo_retriever,retriever_debug_index: None,n_docs: 5,min_doc_token_length: 64,max_doc_token_length: 256,rag_query_truncate: 512,print_docs: False,path_to_index: zoo:hallucination/wiki_index_compressed/compressed_pq,path_to_dense_embeddings: None,dpr_model_file: zoo:hallucination/multiset_dpr/hf_bert_base.cp,path_to_dpr_passages: zoo:hallucination/wiki_passages/psgs_w100.tsv,retriever_embedding_size: 768,tfidf_max_doc_paragraphs: -1,tfidf_model_path: zoo:wikipedia_full/tfidf_retriever/model,dpr_num_docs: 25,poly_score_initial_lambda: 0.5,polyencoder_init_model: wikito,poly_faiss_model_file: None,regret: False,regret_intermediate_maxlen: 32,regret_model_file: None,regret_dict_file: None,regret_override_index: False,indexer_type: compressed,indexer_buffer_size: 65536,compressed_indexer_factory: IVF4096_HNSW128,PQ128,compressed_indexer_gpu_train: False,compressed_indexer_nprobe: 64,hnsw_indexer_store_n: 128,hnsw_ef_search: 128,hnsw_ef_construction: 200,rag_turn_n_turns: 2,rag_turn_marginalize: doc_then_turn,rag_turn_discount_factor: 1.0,t5_model_arch: t5-base,t5_model_parallel: False,t5_dropout: 0.0,t5_generation_config: None,search_query_generator_model_file: None,search_query_generator_inference: greedy,search_query_generator_beam_min_length: 1,search_query_generator_beam_size: 1,search_query_generator_text_truncate: 512,splitted_chunk_length: 256,doc_chunk_split_mode: word,n_ranked_doc_chunks: 1,doc_chunks_ranker: head,woi_doc_chunk_size: 500,skip_retrieval_key: skip_retrieval,serializable: False,rank: 0,dict_loaded: True,datapath: /usr/local/lib/python3.10/site-packages/data\u001b[0m\n",
      "14:08:48 | \u001b[33myour model is being loaded with opts that differ from the model you are initializing the weights with. Add the following args to your run command to change this: \n",
      "--init-opt None --task blended_skill_talk,wizard_of_wikipedia:Generator,convai2:normalized,empathetic_dialogues --multitask-weights 1.0,3.0,3.0,3.0 --batchsize 2 --dynamic-batching full --model bart --beam-size 10 --beam-min-length 20 --beam-context-block-ngram 3 --beam-block-ngram 3 --skip-generation False --inference beam --truncate 1022 --text-truncate 1000 --label-truncate 1000\u001b[0m\n",
      "14:08:48 | loading dictionary from /usr/local/lib/python3.10/site-packages/data/models/bb3/bb3_3B/model.dict\n",
      "14:08:48 | num words = 50264\n",
      "14:09:15 | Total parameters: 2,692,276,224 (2,692,276,224 trainable)\n",
      "14:09:15 | Loading existing model params from /usr/local/lib/python3.10/site-packages/data/models/bb3/bb3_3B/model\n",
      "You said: your persona: I like cheese\n",
      "your persona: I am from New York City\n",
      "Hi, where are you from\n",
      "14:09:54 | \u001b[33m--skip-generation true produces limited metrics\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# model produces a response\u001b[39;00m\n\u001b[1;32m     21\u001b[0m startTime \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 22\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mblender_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBlenderBot replied: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m startTime)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/parlai/core/torch_agent.py:2149\u001b[0m, in \u001b[0;36mTorchAgent.act\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2145\u001b[0m \u001b[38;5;66;03m# BatchWorld handles calling self_observe, but we're in a Hogwild or Interactive\u001b[39;00m\n\u001b[1;32m   2146\u001b[0m \u001b[38;5;66;03m# world, so we need to handle this ourselves.\u001b[39;00m\n\u001b[1;32m   2148\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_act([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation])[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m-> 2149\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_observe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/parlai/core/torch_agent.py:1926\u001b[0m, in \u001b[0;36mTorchAgent.self_observe\u001b[0;34m(self, self_message)\u001b[0m\n\u001b[1;32m   1921\u001b[0m     \u001b[38;5;66;03m# you might expect a hard failure here, but in interactive mode we'll\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m     \u001b[38;5;66;03m# never get a label\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# otherwise, we use the last output the model generated\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m self_message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1926\u001b[0m     last_reply \u001b[38;5;241m=\u001b[39m \u001b[43mself_message\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory\u001b[38;5;241m.\u001b[39madd_reply(last_reply)\n\u001b[1;32m   1928\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from parlai.core.agents import create_agent_from_model_file\n",
    "# blender_agent = create_agent_from_model_file(\"zoo:blender/blender_400M/model\")\n",
    "# blender_agent = create_agent_from_model_file(\"zoo:blender/blender_3B/model\")\n",
    "blender_agent = create_agent_from_model_file(\"zoo:bb3/bb3_3B/model\")\n",
    "\n",
    "# forget everything. Important if you run this multiple times.\n",
    "blender_agent.reset()\n",
    "\n",
    "# concatenate the persona and the first thing the human says\n",
    "first_turn = \"\\n\".join([\n",
    "    \"your persona: I like cheese\",\n",
    "    \"your persona: I am from New York City\",\n",
    "    \"Hi, where are you from\"\n",
    "])\n",
    "# Model actually witnesses the human's text\n",
    "blender_agent.observe({'text': first_turn, 'episode_done': False})\n",
    "print(f\"You said: {first_turn}\")\n",
    "\n",
    "# model produces a response\n",
    "startTime = time.time()\n",
    "response = blender_agent.act()\n",
    "print(\"BlenderBot replied: {}\".format(response['text']))\n",
    "print(time.time() - startTime)\n",
    "print()\n",
    "\n",
    "# now another turn\n",
    "second_turn = \"do you like cheese?\"\n",
    "print(f\"You said: {second_turn}\")\n",
    "\n",
    "startTime = time.time()\n",
    "blender_agent.observe({'text': second_turn, \"episode_done\": False})\n",
    "response2 = blender_agent.act()\n",
    "print(\"BlenderBot replied: {}\".format(response2['text']))\n",
    "print(time.time() - startTime)\n",
    "\n",
    "print()\n",
    "print('-' * 40)\n",
    "print()\n",
    "print(\"BlenderBot's history view:\")\n",
    "print(blender_agent.history.get_history_str())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "419af1e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlenderBot replied: I've never been to New Jersey, but I've heard it's a nice place to visit.\n",
      "33.50209093093872\n",
      "BlenderBot's history view:\n",
      "your persona: I like cheese\n",
      "your persona: I am from New York City\n",
      "Hi, where are you from  I'm from the city of New York, the most populous city in the United States.  do you like cheese?  Yes, I love cheese.  It's one of my favorite foods.  What about you?  how long do you take to go to the Jersey City?  I've never been to New Jersey, but I've heard it's a nice place to visit.\n"
     ]
    }
   ],
   "source": [
    "# more turns from CJ\n",
    "# third_turn = \"do you know what is piave?\"\n",
    "third_turn = \"how long do you take to go to the Jersey City?\"\n",
    "\n",
    "startTime = time.time()\n",
    "blender_agent.observe({'text': third_turn, \"episode_done\": False})\n",
    "response3 = blender_agent.act()\n",
    "print(\"BlenderBot replied: {}\".format(response3['text']))\n",
    "print(time.time() - startTime)\n",
    "\n",
    "print(\"BlenderBot's history view:\")\n",
    "print(blender_agent.history.get_history_str())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad9d2d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'TransformerGenerator',\n",
       " 'episode_done': False,\n",
       " 'text': \"I've never been to New Jersey, but I've heard it's a nice place to visit.\",\n",
       " 'beam_texts': [(\"I've never been to New Jersey, but I've heard it's a nice place to visit.\",\n",
       "   -6.780792713165283),\n",
       "  (\"I've never been to New Jersey, but I hear it's nice.  I've been to Philadelphia, though.\",\n",
       "   -7.047243595123291),\n",
       "  (\"I've never been, but I'd love to go.  I hear it's the most densely populated city in New York State.\",\n",
       "   -7.1058454513549805),\n",
       "  (\"I've never been to New Jersey, but I hear it's nice.  I've been to Philadelphia, which is the second-largest city in Pennsylvania.\",\n",
       "   -7.14259672164917),\n",
       "  (\"I've never been to New Jersey, but I hear it's nice.  I've been to Philadelphia though.\",\n",
       "   -7.26740026473999),\n",
       "  (\"I've never been, but I'd love to go.  I hear it's the most densely populated state in the US.\",\n",
       "   -7.279247283935547),\n",
       "  (\"I've never been, but I'd love to go.  I hear it's the most densely populated city in America.\",\n",
       "   -7.352784156799316),\n",
       "  (\"I've never been to New Jersey, but I hear it's nice.  I've been to Philadelphia, which is also in the New York metropolitan area.\",\n",
       "   -7.391134262084961),\n",
       "  (\"I've never been, but I'd love to go.  I hear it's the most densely populated urban area in the US.\",\n",
       "   -7.455860137939453),\n",
       "  (\"I've never been, but I'd love to go.  I hear it's the most densely populated urban area in the world.\",\n",
       "   -7.512598991394043),\n",
       "  (\"I've never been to New Jersey, but I hear it's nice.  I've been to Philadelphia, which is the second-largest city in New York State.\",\n",
       "   -7.687839031219482),\n",
       "  (\"I've never been to New Jersey, but I hear it's nice.  I've been to Philadelphia, which is the second-largest city in PA.\",\n",
       "   -7.730276584625244),\n",
       "  (\"I've never been to New Jersey, but I hear it's nice.  I've been to Philadelphia, which is the second largest city in New York State.\",\n",
       "   -7.928862571716309),\n",
       "  (\"I've never been to New Jersey, but I hear it's nice.  I've been to Philadelphia, though, and it's a great city.\",\n",
       "   -7.980947017669678),\n",
       "  (\"I've never been to New Jersey, but I hear it's nice.  I've been to Philadelphia, though, which is the second-largest city in New York State.\",\n",
       "   -7.99509334564209),\n",
       "  (\"I've never been to New Jersey, but I hear it's nice.  I've been to Philadelphia, which is the second-largest city in New York.\",\n",
       "   -8.067826271057129),\n",
       "  (\"I've never been to New Jersey, but I hear it's nice.  I've been to Philadelphia, which is also in New York.\",\n",
       "   -8.07827377319336),\n",
       "  (\"I've never been to New Jersey, but I hear it's nice.  I've been to Philadelphia, which is the second-largest city.\",\n",
       "   -8.090593338012695),\n",
       "  (\"I've never been to New Jersey, but I hear it's nice.  I've been to Philadelphia, though, which is the second-largest city in PA.\",\n",
       "   -8.093864440917969),\n",
       "  (\"I've never been to New Jersey, but I hear it's nice.  I've been to Philadelphia, which is also in the New York metro area.\",\n",
       "   -8.117423057556152),\n",
       "  (\"I've never been to New Jersey, but I hear it's nice.  I've been to Philadelphia, which is the second-largest city in New York state.\",\n",
       "   -8.121554374694824),\n",
       "  (\"I've never been to New Jersey, but I hear it's nice.  I've been to Philadelphia, which is the second largest city.\",\n",
       "   -8.177988052368164),\n",
       "  (\"I've never been to New Jersey, but I hear it's nice.  I've been to Philadelphia, which is also in the New York metropolitan area. \",\n",
       "   -8.185507774353027),\n",
       "  (\"I've never been to New Jersey, but I hear it's nice.  I've been to Philadelphia, which is the second largest city in New York.\",\n",
       "   -8.254595756530762),\n",
       "  (\"I've never been to New Jersey, but I hear it's nice.  I've been to Philadelphia, which is the second-largest city in Pennsylvania\",\n",
       "   -8.348832130432129),\n",
       "  (\"I've never been to New Jersey, but I hear it's nice.  I've been to Philadelphia, which is the second-largest city in New York State. \",\n",
       "   -8.578437805175781)],\n",
       " 'metrics': {'clen': AverageMetric(94),\n",
       "  'ctrunc': AverageMetric(0),\n",
       "  'ctrunclen': AverageMetric(0),\n",
       "  'gen_n_toks': AverageMetric(23)}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d0bfc4",
   "metadata": {},
   "source": [
    "# Use Conversational Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "74d12a33",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "401 Client Error: Unauthorized for url: https://huggingface.co/api/models/metaai/bb3-3B",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mj/8vz4yhqj5w76hnkw54qktw900000gn/T/ipykernel_18604/2528584533.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# modelName = \"facebook/blenderbot-3B\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodelName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"metaai/bb3-3B\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForSeq2SeqLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConversationalPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/models/auto/tokenization_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m                 config = AutoConfig.from_pretrained(\n\u001b[0;32m--> 481\u001b[0;31m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m                 )\n\u001b[1;32m    483\u001b[0m             \u001b[0mconfig_tokenizer_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name_or_path\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0mtrust_remote_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trust_remote_code\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m         \u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"auto_map\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"AutoConfig\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"auto_map\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtrust_remote_code\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m                 \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m                 \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m                 \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m             )\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/configuration_utils.py\u001b[0m in \u001b[0;36mget_configuration_file\u001b[0;34m(path_or_repo, revision, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;31m# Inspect all files from the repo/folder.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     all_files = get_list_of_files(\n\u001b[0;32m--> 842\u001b[0;31m         \u001b[0mpath_or_repo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_files_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlocal_files_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    843\u001b[0m     )\n\u001b[1;32m    844\u001b[0m     \u001b[0mconfiguration_files_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mget_list_of_files\u001b[0;34m(path_or_repo, revision, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m   1950\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1951\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1952\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist_repo_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_repo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1953\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mlist_repo_files\u001b[0;34m(self, repo_id, revision, repo_type, token, timeout)\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrepo_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             info = self.model_info(\n\u001b[0;32m--> 603\u001b[0;31m                 \u001b[0mrepo_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m             )\n\u001b[1;32m    605\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mrepo_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"dataset\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mmodel_info\u001b[0;34m(self, repo_id, revision, token, timeout)\u001b[0m\n\u001b[1;32m    584\u001b[0m         )\n\u001b[1;32m    585\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m         \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mModelInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 953\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/models/metaai/bb3-3B"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Conversation, ConversationalPipeline\n",
    "from flask import Flask\n",
    "from flask import request, jsonify\n",
    "\n",
    "# modelName = \"facebook/blenderbot-1B-distill\"\n",
    "modelName = \"facebook/blenderbot-3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(modelName)\n",
    "nlp = ConversationalPipeline(model=model, tokenizer=tokenizer)\n",
    "\n",
    "conversation = Conversation()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "eb704c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(text):\n",
    "    conversation.add_user_input(text)\n",
    "    result = nlp([conversation], do_sample=False, max_length=1000)\n",
    "    \n",
    "    print(str(result))\n",
    "    print(f\">>>> {list(result.iter_texts())[-1]}\")\n",
    "    *_, last = result.iter_texts()\n",
    "    print(last)\n",
    "#     for is_user, text in result.iter_texts():\n",
    "#         who = \"Me:\" if is_user else \"Bot:\"\n",
    "#         print(f\"{who} {text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a5d4ea32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset():\n",
    "    global conversation\n",
    "    conversation = Conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "16db8ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def persona(text):\n",
    "    conversation.add_user_input('Hello')\n",
    "    conversation.append_response(text)\n",
    "        \n",
    "    # Put the user's messages as \"old message\".\n",
    "    conversation.mark_processed()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "a007f5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset()\n",
    "persona(\"I live in New York City.  I like cheese.  I jog everyday.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "b972e6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (160 > 128). Running this sequence through the model will result in indexing errors\n",
      "Trimmed input from conversation as it was longer than 128 tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation id: 313db9c1-d9d7-4686-8d85-217e69f72982 \n",
      "user >> Hello \n",
      "bot >> I live in New York City.  I like cheese.  I jog everyday. \n",
      "user >> Hello.  How are you? \n",
      "bot >>  I'm good.  Just got back from a jog in the park.. \n",
      "user >> Wich park was it? \n",
      "bot >>  It was the Brooklyn Bridge Park.  It's a nice place to jog. \n",
      "user >> Where is the Brooklyn Bridge Park? \n",
      "bot >>  It is located in Manhattan, New York, United States. \n",
      "user >> What are the streets? \n",
      "bot >>  I'm not sure, but I do know that it is the most visited park in the world. \n",
      "user >> You jogged there but you don't know the streets?  Hard to believe \n",
      "bot >>  Yes, it is hard to believe.  But it is true.  The park was built in 1898. \n",
      "\n",
      ">>>> (False, ' Yes, it is hard to believe.  But it is true.  The park was built in 1898.')\n",
      "(False, ' Yes, it is hard to believe.  But it is true.  The park was built in 1898.')\n",
      "None\n",
      "27.04387092590332\n"
     ]
    }
   ],
   "source": [
    "startTime = time.time()\n",
    "print(chat(\"You jogged there but you don't know the streets?  Hard to believe\"))\n",
    "print(time.time() - startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b80988f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " 'add_user_input',\n",
       " 'append_response',\n",
       " 'generated_responses',\n",
       " 'iter_texts',\n",
       " 'mark_processed',\n",
       " 'new_user_input',\n",
       " 'past_user_inputs',\n",
       " 'uuid']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.add_user_input(\"I like NYC\")\n",
    "result = nlp([conversation], do_sample=False, max_length=1000)\n",
    "dir(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
